{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa855358",
      "metadata": {
        "id": "aa855358"
      },
      "source": [
        "# Exercise-1: Training Deep Neural Network on MNIST\n",
        "\n",
        "Train a controlled deep neural network on the MNIST dataset. Set random seeds to 42.\n",
        "Load and preprocess MNIST. Build the network using the following configuration:\n",
        "* Flatten input images to 28 × 28 = 784 features\n",
        "* 3 hidden layers, 64 neurons each\n",
        "* ELU activation function\n",
        "* He normal initialization\n",
        "* Output layer: 10 neurons with softmax\n",
        "* Optimizer: Nadam\n",
        "* learning rate = 0.001, loss=sparse categorical crossentropy\n",
        "* EarlyStopping callback: monitor validation loss, patience = 5, restore best weights\n",
        "* epochs = 50, batch size = 32\n",
        "* Use only the first 1000 training samples and first 200 test samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "700c478f",
      "metadata": {
        "id": "700c478f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1196e15f",
      "metadata": {
        "id": "1196e15f"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "94b3a726",
      "metadata": {
        "id": "94b3a726"
      },
      "outputs": [],
      "source": [
        "# 1. Set seeds\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4a43868d",
      "metadata": {
        "id": "4a43868d",
        "outputId": "0aaea0e2-87b1-4eb2-9e5e-5bd603da4229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 537kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.56MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.70MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 1.4510 | Val Loss: 0.8960\n",
            "Epoch 02 | Train Loss: 0.5706 | Val Loss: 0.5384\n",
            "Epoch 03 | Train Loss: 0.3566 | Val Loss: 0.4145\n",
            "Epoch 04 | Train Loss: 0.2649 | Val Loss: 0.3699\n",
            "Epoch 05 | Train Loss: 0.1881 | Val Loss: 0.3231\n",
            "Epoch 06 | Train Loss: 0.1341 | Val Loss: 0.3018\n",
            "Epoch 07 | Train Loss: 0.1017 | Val Loss: 0.3012\n",
            "Epoch 08 | Train Loss: 0.0732 | Val Loss: 0.2884\n",
            "Epoch 09 | Train Loss: 0.0528 | Val Loss: 0.2864\n",
            "Epoch 10 | Train Loss: 0.0383 | Val Loss: 0.2985\n",
            "Epoch 11 | Train Loss: 0.0275 | Val Loss: 0.3177\n",
            "Epoch 12 | Train Loss: 0.0225 | Val Loss: 0.3262\n",
            "Epoch 13 | Train Loss: 0.0171 | Val Loss: 0.3215\n",
            "Epoch 14 | Train Loss: 0.0143 | Val Loss: 0.3109\n",
            "Early stopping triggered.\n",
            "Restored best model weights.\n"
          ]
        }
      ],
      "source": [
        "# 2. Load MNIST (only part)\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_subset = Subset(train_dataset, range(1000))   # first 1000 samples\n",
        "test_subset  = Subset(test_dataset, range(200))     # first 200 samples\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 3. Build model\n",
        "\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(784, 64),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(64, 10)  # logits -> CrossEntropyLoss will softmax\n",
        "        )\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_normal_(layer.weight)  # He normal init\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MNISTModel()\n",
        "\n",
        "# 4. Optimizer & loss\n",
        "\n",
        "optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 5. Training with Early Stopping\n",
        "\n",
        "epochs = 50\n",
        "patience = 5\n",
        "best_loss = np.inf\n",
        "patience_counter = 0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # valdiation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_state = model.state_dict().copy()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best weights\n",
        "model.load_state_dict(best_state)\n",
        "print(\"Restored best model weights.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c74fe5d",
      "metadata": {
        "id": "6c74fe5d"
      },
      "source": [
        "### Q1.1 Report the obtained test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5c358bcd",
      "metadata": {
        "id": "5c358bcd",
        "outputId": "53312e23-978f-4c3e-c0fa-1c8110336c92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 92.00%\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        preds = model(x).argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "print(f\"Accuracy: {correct/total:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2f18c0",
      "metadata": {
        "id": "8e2f18c0"
      },
      "source": [
        "# Exercise-2: Training Deep Neural Network on CIFAR-10\n",
        "Train a controlled deep neural network on the CIFAR-10 dataset. Set random seeds to\n",
        "42. Load and preprocess CIFAR-10. Build the network using the following configuration:\n",
        "* Flatten input images to 32 × 32 × 3 = 3072 features\n",
        "* 4 hidden layers, 256 neurons each\n",
        "* ELU activation function\n",
        "* He normal initialization\n",
        "* Output layer: 10 neurons with softmax\n",
        "* Optimizer: Nadam\n",
        "* learning rate = 0.001, loss =′ sparse categorical crossentropy′\n",
        "* EarlyStopping callback: monitor validation loss, patience = 5, restore best weights\n",
        "* epochs = 50, batch size = 128\n",
        "* Use only the first 5000 training samples and first 1000 test samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b69ea1c0",
      "metadata": {
        "id": "b69ea1c0",
        "outputId": "2da97476-768c-4a2c-a797-2fc42be54c58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 33.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 3.2324 | Val Loss: 2.7083\n",
            "Epoch 02 | Train Loss: 2.1901 | Val Loss: 2.4268\n",
            "Epoch 03 | Train Loss: 2.0484 | Val Loss: 2.7258\n",
            "Epoch 04 | Train Loss: 2.0169 | Val Loss: 3.0117\n",
            "Epoch 05 | Train Loss: 1.9854 | Val Loss: 2.8556\n",
            "Epoch 06 | Train Loss: 1.9054 | Val Loss: 2.3269\n",
            "Epoch 07 | Train Loss: 1.8354 | Val Loss: 2.6527\n",
            "Epoch 08 | Train Loss: 1.8477 | Val Loss: 2.8217\n",
            "Epoch 09 | Train Loss: 1.9537 | Val Loss: 2.2374\n",
            "Epoch 10 | Train Loss: 1.7636 | Val Loss: 2.6508\n",
            "Epoch 11 | Train Loss: 1.7576 | Val Loss: 2.9835\n",
            "Epoch 12 | Train Loss: 1.7567 | Val Loss: 3.0352\n",
            "Epoch 13 | Train Loss: 1.7366 | Val Loss: 2.0966\n",
            "Epoch 14 | Train Loss: 1.6261 | Val Loss: 2.3295\n",
            "Epoch 15 | Train Loss: 1.6397 | Val Loss: 2.3083\n",
            "Epoch 16 | Train Loss: 1.5778 | Val Loss: 3.0284\n",
            "Epoch 17 | Train Loss: 1.6641 | Val Loss: 2.2313\n",
            "Epoch 18 | Train Loss: 1.5161 | Val Loss: 4.8174\n",
            "Early stopping triggered.\n",
            "Restored best model weights.\n"
          ]
        }
      ],
      "source": [
        "# 1. Set random (already done above)\n",
        "# 2. Load CIFAR-10 (only part)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_subset = Subset(train_dataset, range(5000))   # first 5000 samples\n",
        "test_subset  = Subset(test_dataset, range(1000))    # first 1000 samples\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 3. Build Model\n",
        "\n",
        "class CIFAR10MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3072, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 10)  # logits (softmax done in CrossEntropy)\n",
        "        )\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.model:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_normal_(layer.weight)  # He init\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = CIFAR10MLP()\n",
        "\n",
        "# 4. Optimizer & loss\n",
        "\n",
        "optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 5. Train w/ Early Stopping\n",
        "\n",
        "epochs = 50\n",
        "patience = 5\n",
        "best_loss = np.inf\n",
        "patience_counter = 0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_state = model.state_dict().copy()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best weights\n",
        "model.load_state_dict(best_state)\n",
        "print(\"Restored best model weights.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d6fe79",
      "metadata": {
        "id": "07d6fe79"
      },
      "source": [
        "### Q2.1 Report the obtained the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "854a1bf7",
      "metadata": {
        "id": "854a1bf7",
        "outputId": "82c233a2-0e40-45c4-bedb-0eaaade7da9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 12.90%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        preds = model(x).argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "print(f\"Final Test Accuracy: {correct/total:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ba3fe4",
      "metadata": {
        "id": "a6ba3fe4"
      },
      "source": [
        "# Exercise-3: Regularization with Alpha Dropout and MC Dropout\n",
        "Using the MNIST dataset, extend the previously trained deep neural network by applying\n",
        "Alpha Dropout. Then, without retraining, use Monte Carlo (MC) Dropout at inference\n",
        "to estimate if you can achieve better accuracy. Set random seeds to 42. Use the following\n",
        "configuration:\n",
        "* Flatten input images to 28 × 28 = 784 features\n",
        "* 3 hidden layers, 64 neurons each\n",
        "* SELU activation function (required for Alpha Dropout)\n",
        "* LeCun normal initialization\n",
        "* Alpha Dropout rate: 0.1 in all hidden layers\n",
        "* Output layer: 10 neurons with softmax\n",
        "* Optimizer: Nadam\n",
        "* learning rate = 0.001, loss=sparse categorical crossentropy\n",
        "* epochs = 50, batch size = 32\n",
        "* Use only the first 1000 training samples and first 200 test samples\n",
        "* For MC Dropout, enable dropout during inference and average predictions over 20 stochastic forward passes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "10a914b3",
      "metadata": {
        "id": "10a914b3",
        "outputId": "89c8330c-6432-4770-8455-9f1a6026a4fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Loss: 0.4412\n",
            "Epoch 02 | Loss: 0.2275\n",
            "Epoch 03 | Loss: 0.1765\n",
            "Epoch 04 | Loss: 0.1456\n",
            "Epoch 05 | Loss: 0.1287\n",
            "Epoch 06 | Loss: 0.1149\n",
            "Epoch 07 | Loss: 0.1068\n",
            "Epoch 08 | Loss: 0.0976\n",
            "Epoch 09 | Loss: 0.0902\n",
            "Epoch 10 | Loss: 0.0858\n",
            "Epoch 11 | Loss: 0.0815\n",
            "Epoch 12 | Loss: 0.0781\n",
            "Epoch 13 | Loss: 0.0736\n",
            "Epoch 14 | Loss: 0.0712\n",
            "Epoch 15 | Loss: 0.0688\n",
            "Epoch 16 | Loss: 0.0673\n",
            "Epoch 17 | Loss: 0.0634\n",
            "Epoch 18 | Loss: 0.0620\n",
            "Epoch 19 | Loss: 0.0608\n",
            "Epoch 20 | Loss: 0.0585\n",
            "Epoch 21 | Loss: 0.0569\n",
            "Epoch 22 | Loss: 0.0539\n",
            "Epoch 23 | Loss: 0.0557\n",
            "Epoch 24 | Loss: 0.0513\n",
            "Epoch 25 | Loss: 0.0526\n",
            "Epoch 26 | Loss: 0.0512\n",
            "Epoch 27 | Loss: 0.0508\n",
            "Epoch 28 | Loss: 0.0475\n",
            "Epoch 29 | Loss: 0.0471\n",
            "Epoch 30 | Loss: 0.0487\n",
            "Epoch 31 | Loss: 0.0455\n",
            "Epoch 32 | Loss: 0.0463\n",
            "Epoch 33 | Loss: 0.0448\n",
            "Epoch 34 | Loss: 0.0435\n",
            "Epoch 35 | Loss: 0.0443\n",
            "Epoch 36 | Loss: 0.0439\n",
            "Epoch 37 | Loss: 0.0433\n",
            "Epoch 38 | Loss: 0.0410\n",
            "Epoch 39 | Loss: 0.0404\n",
            "Epoch 40 | Loss: 0.0419\n",
            "Epoch 41 | Loss: 0.0411\n",
            "Epoch 42 | Loss: 0.0380\n",
            "Epoch 43 | Loss: 0.0392\n",
            "Epoch 44 | Loss: 0.0387\n",
            "Epoch 45 | Loss: 0.0379\n",
            "Epoch 46 | Loss: 0.0368\n",
            "Epoch 47 | Loss: 0.0361\n",
            "Epoch 48 | Loss: 0.0378\n",
            "Epoch 49 | Loss: 0.0377\n",
            "Epoch 50 | Loss: 0.0377\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# 2. Load MNIST\n",
        "# -------------------------\n",
        "transform = transforms.ToTensor()\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "# -------------------------\n",
        "# 3. Define Model (SELU + AlphaDropout)\n",
        "# -------------------------\n",
        "class MNISTAlphaDropoutNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(784, 64),\n",
        "            nn.SELU(),\n",
        "            nn.AlphaDropout(p=0.1),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.SELU(),\n",
        "            nn.AlphaDropout(p=0.1),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.SELU(),\n",
        "            nn.AlphaDropout(p=0.1),\n",
        "            nn.Linear(64, 10)  # logits\n",
        "        )\n",
        "        self.init_lecun_normal()\n",
        "\n",
        "    def init_lecun_normal(self):\n",
        "        for layer in self.model:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                fan_in = layer.weight.shape[1]\n",
        "                nn.init.normal_(layer.weight, mean=0.0, std=np.sqrt(1.0 / fan_in))\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = MNISTAlphaDropoutNet()\n",
        "\n",
        "# -------------------------\n",
        "# 4. Optimizer & loss\n",
        "# -------------------------\n",
        "optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# -------------------------\n",
        "# 5. Train normally\n",
        "# -------------------------\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # valdiation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_state = model.state_dict().copy()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# -------------------------\n",
        "# 6. Standard accuracy (dropout OFF)\n",
        "# -------------------------\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            preds = model(x).argmax(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "468f013b",
      "metadata": {
        "id": "468f013b"
      },
      "source": [
        "## Q3.1 Report the test accuracy of the network with Alpha Dropout applied during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6b29cd03",
      "metadata": {
        "id": "6b29cd03",
        "outputId": "6074b2c3-1b4b-47a0-d355-5cdcf43d4350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard Test Accuracy (Dropout OFF): 98.0600%\n"
          ]
        }
      ],
      "source": [
        "base_acc = evaluate(model)\n",
        "print(f\"Standard Test Accuracy (Dropout OFF): {base_acc:.4%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c126866b",
      "metadata": {
        "id": "c126866b"
      },
      "source": [
        "## Q3.2 Report the MC Dropout-enhanced accuracy (averaging 20 stochastic predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8a9ab5f3",
      "metadata": {
        "id": "8a9ab5f3",
        "outputId": "7bcce778-a63d-4758-8227-022835940991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MC Dropout Test Accuracy (Dropout ON, 20 passes): 97.9800%\n"
          ]
        }
      ],
      "source": [
        "def mc_dropout_predict(model, x, mc_runs=20):\n",
        "    model.train()  # <— enable dropout at inference\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(mc_runs):\n",
        "            logits = model(x)\n",
        "            preds.append(torch.softmax(logits, dim=1))\n",
        "    return torch.stack(preds).mean(0)  # average prediction\n",
        "\n",
        "\n",
        "def evaluate_mc_dropout(model, mc_runs=20):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # we handle dropout manually inside function\n",
        "    for x, y in test_loader:\n",
        "        probs = mc_dropout_predict(model, x, mc_runs)\n",
        "        pred = probs.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "mc_acc = evaluate_mc_dropout(model)\n",
        "print(f\"MC Dropout Test Accuracy (Dropout ON, 20 passes): {mc_acc:.4%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7cac7b2",
      "metadata": {
        "id": "d7cac7b2"
      },
      "source": [
        "# Exercise-4: Transfer Learning with Pre-trained CNN\n",
        "Use a pre-trained convolutional neural network (CNN) as a feature extractor and fine-\n",
        "tune a classifier on a subset of the CIFAR-10 dataset. Set random seeds to 42. Follow\n",
        "the configuration below:\n",
        "* Load CIFAR-10 and normalize pixel values to [0,1]\n",
        "* Use only the first 2000 training samples and first 500 test samples\n",
        "* Load MobileNetV2 from tensorflow.keras.applications, with include top=False and weights=’imagenet’\n",
        "* Freeze all layers of the pre-trained base\n",
        "* Add a classifier on top:\n",
        "    * GlobalAveragePooling2D\n",
        "    * Dense layer with 128 neurons, ReLU activation\n",
        "    * Dropout: 0.2\n",
        "    * Output layer: 10 neurons with softmax\n",
        "* Optimizer: Adam, learning rate = 0.001\n",
        "* Loss: sparse categorical crossentropy\n",
        "* epochs = 5, batch size = 32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 1. CIFAR-10 dataset\n",
        "# ---------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),    # MobileNet requires 224x224\n",
        "    transforms.ToTensor(),            # Scales to [0,1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]), # ImageNet stats\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_subset = Subset(train_dataset, range(2000))\n",
        "test_subset = Subset(test_dataset, range(500))\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Load MobileNetV2, remove top, freeze base\n",
        "# ---------------------------\n",
        "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "\n",
        "for p in mobilenet.features.parameters():  # freeze backbone only\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Replace classifier to mimic Keras: GAP → Dense128 → Dropout → Dense10\n",
        "# (BUT PyTorch already does GAP before classifier)\n",
        "mobilenet.classifier = nn.Sequential(\n",
        "    nn.Linear(1280, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128, 10)  # Softmax removed (CrossEntropyLoss handles it)\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "mobilenet.to(device)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Training setup\n",
        "# ---------------------------\n",
        "criterion = nn.CrossEntropyLoss()  # sparse categorical crossentropy\n",
        "optimizer = optim.Adam(mobilenet.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Train\n",
        "# ---------------------------\n",
        "for epoch in range(5):\n",
        "    mobilenet.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mobilenet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/5, Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "gnxz67qUZc1g",
        "outputId": "144c74cf-507e-4eaa-cb71-2319085537e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gnxz67qUZc1g",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 1.6089\n",
            "Epoch 2/5, Loss: 1.0652\n",
            "Epoch 3/5, Loss: 0.8992\n",
            "Epoch 4/5, Loss: 0.7768\n",
            "Epoch 5/5, Loss: 0.7176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4.1 Report the test accuracy of the model"
      ],
      "metadata": {
        "id": "Ut6VVSYTZkeU"
      },
      "id": "Ut6VVSYTZkeU"
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = mobilenet(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {(correct/total)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "qzUGqQFlZokZ",
        "outputId": "b9305e73-b240-4525-e317-6e49ba8d0c91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qzUGqQFlZokZ",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 69.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise-5: Deeper CNN Training on SVHN\n",
        "Train a controlled deep convolutional neural network (CNN) on a subset of the SVHN\n",
        "dataset. Set random seeds to 42. Load and preprocess SVHN. Build the network using\n",
        "the following configuration:\n",
        "* Load SVHN and normalize pixel values to [0,1]\n",
        "* Use only the first 2000 training samples and first 500 test samples\n",
        "* Input shape: 32 × 32 × 3\n",
        "* CNN architecture:\n",
        "  * Conv2D: 32 filters, 3×3 kernel, ReLU activation\n",
        "  * Conv2D: 32 filters, 3×3 kernel, ReLU activation\n",
        "  * MaxPooling2D: 2×2\n",
        "  * Conv2D: 64 filters, 3×3 kernel, ReLU activation\n",
        "  * Conv2D: 64 filters, 3×3 kernel, ReLU activation\n",
        "  * MaxPooling2D: 2×2 Flatten\n",
        "  * Dense: 256 neurons, ReLU activation\n",
        "  * Dropout: 0.3\n",
        "  * Output layer: 10 neurons with softmax\n",
        "* Optimizer: Adam, learning rate = 0.001\n",
        "* Loss: sparse categorical crossentropy\n",
        "* epochs = 15, batch size = 32"
      ],
      "metadata": {
        "id": "RHn-8LtkdkX6"
      },
      "id": "RHn-8LtkdkX6"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load SVHN (normalize to [0,1])\n",
        "\n",
        "transform = transforms.ToTensor()  # already scales to [0,1]\n",
        "\n",
        "train_data = datasets.SVHN(root=\"./data\", split='train', download=True, transform=transform)\n",
        "test_data  = datasets.SVHN(root=\"./data\", split='test',  download=True, transform=transform)\n",
        "\n",
        "# Use dataset subsets\n",
        "train_subset = Subset(train_data, range(2000))\n",
        "test_subset  = Subset(test_data, range(500))\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 2. Define CNN Model\n",
        "class SVHNCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 256), nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 10)  # logits\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = SVHNCNN()\n",
        "\n",
        "# 3. Optimizer & Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. Training loop\n",
        "epochs = 15\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        # SVHN labels are shape (N,1), squeeze needed\n",
        "        y = y.long().squeeze()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "CLPuLlXge0UD",
        "outputId": "467efbc9-2048-4cd7-e6f1-69c2ebfedac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CLPuLlXge0UD",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 182M/182M [00:04<00:00, 41.8MB/s]\n",
            "100%|██████████| 64.3M/64.3M [00:01<00:00, 36.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Loss: 2.2411\n",
            "Epoch 02 | Loss: 2.2362\n",
            "Epoch 03 | Loss: 2.2295\n",
            "Epoch 04 | Loss: 2.2310\n",
            "Epoch 05 | Loss: 2.2267\n",
            "Epoch 06 | Loss: 2.2348\n",
            "Epoch 07 | Loss: 2.2303\n",
            "Epoch 08 | Loss: 2.2262\n",
            "Epoch 09 | Loss: 2.2299\n",
            "Epoch 10 | Loss: 2.2291\n",
            "Epoch 11 | Loss: 2.2319\n",
            "Epoch 12 | Loss: 2.2216\n",
            "Epoch 13 | Loss: 2.2247\n",
            "Epoch 14 | Loss: 2.2161\n",
            "Epoch 15 | Loss: 2.1852\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5.1 Report the test accuracy of the deeper CNN model."
      ],
      "metadata": {
        "id": "Wu62dnvCd4_W"
      },
      "id": "Wu62dnvCd4_W"
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        y = y.long().squeeze()\n",
        "        preds = model(x).argmax(1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {correct/total:.4%}\")"
      ],
      "metadata": {
        "id": "UTrT29_xe15l",
        "outputId": "d0be15da-0e80-4a0a-e004-0e4b4de0d834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UTrT29_xe15l",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 19.2000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise-6: CNN with SGD, MC Dropout, and Epistemic Uncertainty\n",
        "Train a controlled convolutional neural network (CNN) on a subset of the SVHN dataset\n",
        "using SGD optimizer. Then, apply Monte Carlo (MC) Dropout at inference to estimate\n",
        "both test accuracy and epistemic uncertainty. Set random seeds to 42. Use the following\n",
        "configuration:\n",
        "* Load SVHN and normalize pixel values to [0,1]\n",
        "* Use only the first 2000 training samples and first 500 test samples\n",
        "* Input shape: 32 × 32 × 3\n",
        "* CNN architecture:\n",
        "  * Conv2D: 32 filters, 3×3 kernel, ReLU activation\n",
        "  * MaxPooling2D: 2×2\n",
        "  * Conv2D: 64 filters, 3×3 kernel, ReLU activation\n",
        "  * MaxPooling2D: 2×2\n",
        "  * Flatten\n",
        "  * Dense: 128 neurons, ReLU activation\n",
        "  * Dropout: 0.25 (keep during inference for MC Dropout)\n",
        "  * Output layer: 10 neurons with softmax\n",
        "* Optimizer: SGD with momentum = 0.9, learning rate = 0.01\n",
        "* Loss: sparse categorical crossentropy\n",
        "* epochs = 15, batch size = 32\n",
        "* For MC Dropout:\n",
        "  * Enable dropout during inference\n",
        "  * Average predictions over 20 stochastic forward passes\n",
        "  * Compute the epistemic uncertainty as the predictive variance across passes"
      ],
      "metadata": {
        "id": "bA8wORk7d7yK"
      },
      "id": "bA8wORk7d7yK"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load SVHN (normalize to [0,1])\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.SVHN(root=\"./data\", split='train', download=True, transform=transform)\n",
        "test_data  = datasets.SVHN(root=\"./data\", split='test',  download=True, transform=transform)\n",
        "\n",
        "train_subset = Subset(train_data, range(2000))\n",
        "test_subset  = Subset(test_data, range(500))\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 2. CNN Model\n",
        "class MC_Dropout_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 128), nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, 10)  # logits\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MC_Dropout_CNN()\n",
        "\n",
        "# 3. Optimizer & Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. Training\n",
        "epochs = 15\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        y = y.long().squeeze()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training complete.\\n\")"
      ],
      "metadata": {
        "id": "AMmsLY0UfPCv",
        "outputId": "2c9a1d6c-27ba-49ee-fd20-f0709d58ef0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AMmsLY0UfPCv",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Loss: 2.2465\n",
            "Epoch 02 | Loss: 2.2277\n",
            "Epoch 03 | Loss: 2.2278\n",
            "Epoch 04 | Loss: 2.2226\n",
            "Epoch 05 | Loss: 2.2207\n",
            "Epoch 06 | Loss: 2.2113\n",
            "Epoch 07 | Loss: 2.2043\n",
            "Epoch 08 | Loss: 2.1907\n",
            "Epoch 09 | Loss: 2.1619\n",
            "Epoch 10 | Loss: 2.1213\n",
            "Epoch 11 | Loss: 2.0294\n",
            "Epoch 12 | Loss: 1.8501\n",
            "Epoch 13 | Loss: 1.6025\n",
            "Epoch 14 | Loss: 1.3268\n",
            "Epoch 15 | Loss: 1.1186\n",
            "Training complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.1 Report the plain test accuracy of the CNN trained with SGD (no MC Dropout)."
      ],
      "metadata": {
        "id": "6eLF7I7ZeRGV"
      },
      "id": "6eLF7I7ZeRGV"
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_standard(model):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            y = y.long().squeeze()\n",
        "            preds = model(x).argmax(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "base_acc = eval_standard(model)\n",
        "print(f\"Standard Test Accuracy: {base_acc:.4%}\")"
      ],
      "metadata": {
        "id": "w5zGJifhfZhm",
        "outputId": "c5bb5807-b794-445b-a720-93aa2c701019",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "w5zGJifhfZhm",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard Test Accuracy: 58.8000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.2 Report the MC Dropout-enhanced accuracy (averaging 20 stochastic predictions)."
      ],
      "metadata": {
        "id": "hyKIq7GYeUuQ"
      },
      "id": "hyKIq7GYeUuQ"
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_dropout_predict(model, x, mc_runs=20):\n",
        "    model.train()  # enable dropout for MC inference\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(mc_runs):\n",
        "            logits = model(x)\n",
        "            preds.append(torch.softmax(logits, dim=1))\n",
        "    return torch.stack(preds)  # shape: [mc_runs, batch, 10]\n",
        "\n",
        "def eval_mc_dropout(model, mc_runs=20):\n",
        "    correct, total = 0, 0\n",
        "    uncertainties = []\n",
        "\n",
        "    for x, y in test_loader:\n",
        "        y = y.long().squeeze()\n",
        "\n",
        "        mc_preds = mc_dropout_predict(model, x, mc_runs)\n",
        "        mean_probs = mc_preds.mean(dim=0)  # mean prediction\n",
        "        var_probs = mc_preds.var(dim=0).mean(dim=1)  # avg var per sample (epistemic)\n",
        "\n",
        "        preds = mean_probs.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "        uncertainties.extend(var_probs.cpu().numpy())\n",
        "\n",
        "    return (correct / total), np.array(uncertainties)\n",
        "\n",
        "mc_acc, epistemic_unc = eval_mc_dropout(model, mc_runs=20)\n",
        "\n",
        "print(f\"MC Dropout Test Accuracy: {mc_acc:.4%}\")"
      ],
      "metadata": {
        "id": "vn7gVVLbfaiP",
        "outputId": "ea8479c3-29a1-46ba-af85-db6ca3c9544a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vn7gVVLbfaiP",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MC Dropout Test Accuracy: 59.0000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.3 Compute the average epistemic uncertainty (mean predictive variance) across all test samples. Report it as a deterministic number rounded to 3 decimal places"
      ],
      "metadata": {
        "id": "yyAMN8xIeYuh"
      },
      "id": "yyAMN8xIeYuh"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Epistemic Uncertainty: {epistemic_unc.mean():.6f}\")\n",
        "print(f\"Uncertainty Std: {epistemic_unc.std():.6f}\")"
      ],
      "metadata": {
        "id": "OjvnTZvbfkq5",
        "outputId": "224d14e9-092f-4800-92d1-636666d64fa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OjvnTZvbfkq5",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Epistemic Uncertainty: 0.003791\n",
            "Uncertainty Std: 0.003380\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}