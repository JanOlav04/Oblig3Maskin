{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa855358",
   "metadata": {},
   "source": [
    "# Exercise-1: Training Deep Neural Network on MNIST \n",
    "\n",
    "Train a controlled deep neural network on the MNIST dataset. Set random seeds to 42.\n",
    "Load and preprocess MNIST. Build the network using the following configuration:\n",
    "* Flatten input images to 28 × 28 = 784 features\n",
    "* 3 hidden layers, 64 neurons each\n",
    "* ELU activation function\n",
    "* He normal initialization\n",
    "* Output layer: 10 neurons with softmax\n",
    "* Optimizer: Nadam\n",
    "* learning rate = 0.001, loss=sparse categorical crossentropy\n",
    "* EarlyStopping callback: monitor validation loss, patience = 5, restore best weights\n",
    "* epochs = 50, batch size = 32\n",
    "* Use only the first 1000 training samples and first 200 test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700c478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1196e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94b3a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set seeds\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a43868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 1.4510 | Val Loss: 0.8960\n",
      "Epoch 02 | Train Loss: 0.5706 | Val Loss: 0.5384\n",
      "Epoch 03 | Train Loss: 0.3566 | Val Loss: 0.4145\n",
      "Epoch 04 | Train Loss: 0.2649 | Val Loss: 0.3699\n",
      "Epoch 05 | Train Loss: 0.1881 | Val Loss: 0.3231\n",
      "Epoch 06 | Train Loss: 0.1341 | Val Loss: 0.3018\n",
      "Epoch 07 | Train Loss: 0.1017 | Val Loss: 0.3012\n",
      "Epoch 08 | Train Loss: 0.0732 | Val Loss: 0.2884\n",
      "Epoch 09 | Train Loss: 0.0528 | Val Loss: 0.2864\n",
      "Epoch 10 | Train Loss: 0.0383 | Val Loss: 0.2985\n",
      "Epoch 11 | Train Loss: 0.0275 | Val Loss: 0.3177\n",
      "Epoch 12 | Train Loss: 0.0225 | Val Loss: 0.3262\n",
      "Epoch 13 | Train Loss: 0.0171 | Val Loss: 0.3215\n",
      "Epoch 14 | Train Loss: 0.0143 | Val Loss: 0.3109\n",
      "Early stopping triggered.\n",
      "Restored best model weights.\n"
     ]
    }
   ],
   "source": [
    "# 2. Load MNIST (only part)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_subset = Subset(train_dataset, range(1000))   # first 1000 samples\n",
    "test_subset  = Subset(test_dataset, range(200))     # first 200 samples\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. Build model\n",
    "\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 10)  # logits -> CrossEntropyLoss will softmax\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)  # He normal init\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MNISTModel()\n",
    "\n",
    "# 4. Optimizer & loss\n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 5. Training with Early Stopping\n",
    "\n",
    "epochs = 50\n",
    "patience = 5\n",
    "best_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # valdiation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Restore best weights\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Restored best model weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74fe5d",
   "metadata": {},
   "source": [
    "### Q1.1 Report the obtained test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c358bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        preds = model(x).argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f\"Accuracy: {correct/total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f18c0",
   "metadata": {},
   "source": [
    "# Exercise-2: Training Deep Neural Network on CIFAR-10 \n",
    "Train a controlled deep neural network on the CIFAR-10 dataset. Set random seeds to\n",
    "42. Load and preprocess CIFAR-10. Build the network using the following configuration:\n",
    "* Flatten input images to 32 × 32 × 3 = 3072 features\n",
    "* 4 hidden layers, 256 neurons each\n",
    "* ELU activation function\n",
    "* He normal initialization\n",
    "* Output layer: 10 neurons with softmax\n",
    "* Optimizer: Nadam\n",
    "* learning rate = 0.001, loss =′ sparse categorical crossentropy′\n",
    "* EarlyStopping callback: monitor validation loss, patience = 5, restore best weights\n",
    "* epochs = 50, batch size = 128\n",
    "* Use only the first 5000 training samples and first 1000 test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b69ea1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 3.2465 | Val Loss: 2.7205\n",
      "Epoch 02 | Train Loss: 2.2015 | Val Loss: 2.4690\n",
      "Epoch 03 | Train Loss: 2.0216 | Val Loss: 2.6111\n",
      "Epoch 04 | Train Loss: 2.0074 | Val Loss: 3.0149\n",
      "Epoch 05 | Train Loss: 1.9588 | Val Loss: 2.9991\n",
      "Epoch 06 | Train Loss: 1.9131 | Val Loss: 2.5847\n",
      "Epoch 07 | Train Loss: 1.8907 | Val Loss: 2.6296\n",
      "Early stopping triggered.\n",
      "Restored best model weights.\n"
     ]
    }
   ],
   "source": [
    "# 2. Load CIFAR-10 (only part)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_subset = Subset(train_dataset, range(5000))   # first 5000 samples\n",
    "test_subset  = Subset(test_dataset, range(1000))    # first 1000 samples\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. Build Model\n",
    "\n",
    "class CIFAR10MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3072, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 10)  # logits (softmax done in CrossEntropy)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)  # He init\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = CIFAR10MLP()\n",
    "\n",
    "# 4. Optimizer & loss\n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 5. Train w/ Early Stopping\n",
    "\n",
    "epochs = 50\n",
    "patience = 5\n",
    "best_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Restore best weights\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Restored best model weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6fe79",
   "metadata": {},
   "source": [
    "### Q2.1 Report the obtained the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "854a1bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 17.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        preds = model(x).argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f\"Final Test Accuracy: {correct/total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba3fe4",
   "metadata": {},
   "source": [
    "# Exercise-3: Regularization with Alpha Dropout and MC Dropout\n",
    "Using the MNIST dataset, extend the previously trained deep neural network by applying\n",
    "Alpha Dropout. Then, without retraining, use Monte Carlo (MC) Dropout at inference\n",
    "to estimate if you can achieve better accuracy. Set random seeds to 42. Use the following\n",
    "configuration:\n",
    "* Flatten input images to 28 × 28 = 784 features\n",
    "* 3 hidden layers, 64 neurons each\n",
    "* SELU activation function (required for Alpha Dropout)\n",
    "* LeCun normal initialization\n",
    "* Alpha Dropout rate: 0.1 in all hidden layers\n",
    "* Output layer: 10 neurons with softmax\n",
    "* Optimizer: Nadam\n",
    "* learning rate = 0.001, loss=sparse categorical crossentropy\n",
    "* epochs = 50, batch size = 32\n",
    "* Use only the first 1000 training samples and first 200 test samples\n",
    "* For MC Dropout, enable dropout during inference and average predictions over 20 stochastic forward passes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
