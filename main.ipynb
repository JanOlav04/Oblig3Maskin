{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa855358",
   "metadata": {},
   "source": [
    "# Exercise-1: Training Deep Neural Network on MNIST \n",
    "\n",
    "Train a controlled deep neural network on the MNIST dataset. Set random seeds to 42.\n",
    "Load and preprocess MNIST. Build the network using the following configuration:\n",
    "* Flatten input images to 28 × 28 = 784 features\n",
    "* 3 hidden layers, 64 neurons each\n",
    "* ELU activation function\n",
    "* He normal initialization\n",
    "* Output layer: 10 neurons with softmax\n",
    "* Optimizer: Nadam\n",
    "* learning rate = 0.001, loss=sparse categorical crossentropy\n",
    "* EarlyStopping callback: monitor validation loss, patience = 5, restore best weights\n",
    "* epochs = 50, batch size = 32\n",
    "* Use only the first 1000 training samples and first 200 test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "700c478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94b3a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set seeds\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a43868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 1.4510 | Val Loss: 0.8960\n",
      "Epoch 02 | Train Loss: 0.5706 | Val Loss: 0.5384\n",
      "Epoch 03 | Train Loss: 0.3566 | Val Loss: 0.4145\n",
      "Epoch 04 | Train Loss: 0.2649 | Val Loss: 0.3699\n",
      "Epoch 05 | Train Loss: 0.1881 | Val Loss: 0.3231\n",
      "Epoch 06 | Train Loss: 0.1341 | Val Loss: 0.3018\n",
      "Epoch 07 | Train Loss: 0.1017 | Val Loss: 0.3012\n",
      "Epoch 08 | Train Loss: 0.0732 | Val Loss: 0.2884\n",
      "Epoch 09 | Train Loss: 0.0528 | Val Loss: 0.2864\n",
      "Epoch 10 | Train Loss: 0.0383 | Val Loss: 0.2985\n",
      "Epoch 11 | Train Loss: 0.0275 | Val Loss: 0.3177\n",
      "Epoch 12 | Train Loss: 0.0225 | Val Loss: 0.3262\n",
      "Epoch 13 | Train Loss: 0.0171 | Val Loss: 0.3215\n",
      "Epoch 14 | Train Loss: 0.0143 | Val Loss: 0.3109\n",
      "Early stopping triggered.\n",
      "Restored best model weights.\n"
     ]
    }
   ],
   "source": [
    "# 2. Load MNIST (only part)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_subset = Subset(train_dataset, range(1000))   # first 1000 samples\n",
    "test_subset  = Subset(test_dataset, range(200))     # first 200 samples\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. Build model\n",
    "\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 10)  # logits -> CrossEntropyLoss will softmax\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)  # He normal init\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MNISTModel()\n",
    "\n",
    "# 4. Optimizer & loss\n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 5. Training with Early Stopping\n",
    "\n",
    "epochs = 50\n",
    "patience = 5\n",
    "best_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # valdiation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Restore best weights\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Restored best model weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74fe5d",
   "metadata": {},
   "source": [
    "### Q1.1 Report the obtained test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c358bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        preds = model(x).argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f\"Accuracy: {correct/total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f18c0",
   "metadata": {},
   "source": [
    "# Exercise-2: Training Deep Neural Network on CIFAR-10 \n",
    "Train a controlled deep neural network on the CIFAR-10 dataset. Set random seeds to\n",
    "42. Load and preprocess CIFAR-10. Build the network using the following configuration:\n",
    "* Flatten input images to 32 × 32 × 3 = 3072 features\n",
    "* 4 hidden layers, 256 neurons each\n",
    "* ELU activation function\n",
    "* He normal initialization\n",
    "* Output layer: 10 neurons with softmax\n",
    "* Optimizer: Nadam\n",
    "* learning rate = 0.001, loss =′ sparse categorical crossentropy′\n",
    "* EarlyStopping callback: monitor validation loss, patience = 5, restore best weights\n",
    "* epochs = 50, batch size = 128\n",
    "* Use only the first 5000 training samples and first 1000 test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b69ea1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 3.2465 | Val Loss: 2.7205\n",
      "Epoch 02 | Train Loss: 2.2015 | Val Loss: 2.4690\n",
      "Epoch 03 | Train Loss: 2.0216 | Val Loss: 2.6111\n",
      "Epoch 04 | Train Loss: 2.0074 | Val Loss: 3.0149\n",
      "Epoch 05 | Train Loss: 1.9588 | Val Loss: 2.9991\n",
      "Epoch 06 | Train Loss: 1.9131 | Val Loss: 2.5847\n",
      "Epoch 07 | Train Loss: 1.8907 | Val Loss: 2.6296\n",
      "Early stopping triggered.\n",
      "Restored best model weights.\n"
     ]
    }
   ],
   "source": [
    "# 1. Set random (already done above)\n",
    "# 2. Load CIFAR-10 (only part)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_subset = Subset(train_dataset, range(5000))   # first 5000 samples\n",
    "test_subset  = Subset(test_dataset, range(1000))    # first 1000 samples\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. Build Model\n",
    "\n",
    "class CIFAR10MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3072, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 10)  # logits (softmax done in CrossEntropy)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)  # He init\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = CIFAR10MLP()\n",
    "\n",
    "# 4. Optimizer & loss\n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 5. Train w/ Early Stopping\n",
    "\n",
    "epochs = 50\n",
    "patience = 5\n",
    "best_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Restore best weights\n",
    "model.load_state_dict(best_state)\n",
    "print(\"Restored best model weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6fe79",
   "metadata": {},
   "source": [
    "### Q2.1 Report the obtained the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "854a1bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 17.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        preds = model(x).argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f\"Final Test Accuracy: {correct/total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba3fe4",
   "metadata": {},
   "source": [
    "# Exercise-3: Regularization with Alpha Dropout and MC Dropout\n",
    "Using the MNIST dataset, extend the previously trained deep neural network by applying\n",
    "Alpha Dropout. Then, without retraining, use Monte Carlo (MC) Dropout at inference\n",
    "to estimate if you can achieve better accuracy. Set random seeds to 42. Use the following\n",
    "configuration:\n",
    "* Flatten input images to 28 × 28 = 784 features\n",
    "* 3 hidden layers, 64 neurons each\n",
    "* SELU activation function (required for Alpha Dropout)\n",
    "* LeCun normal initialization\n",
    "* Alpha Dropout rate: 0.1 in all hidden layers\n",
    "* Output layer: 10 neurons with softmax\n",
    "* Optimizer: Nadam\n",
    "* learning rate = 0.001, loss=sparse categorical crossentropy\n",
    "* epochs = 50, batch size = 32\n",
    "* Use only the first 1000 training samples and first 200 test samples\n",
    "* For MC Dropout, enable dropout during inference and average predictions over 20 stochastic forward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a914b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 0.4319\n",
      "Epoch 02 | Loss: 0.2231\n",
      "Epoch 03 | Loss: 0.1738\n",
      "Epoch 04 | Loss: 0.1480\n",
      "Epoch 05 | Loss: 0.1325\n",
      "Epoch 06 | Loss: 0.1148\n",
      "Epoch 07 | Loss: 0.1056\n",
      "Epoch 08 | Loss: 0.0990\n",
      "Epoch 09 | Loss: 0.0911\n",
      "Epoch 10 | Loss: 0.0893\n",
      "Epoch 11 | Loss: 0.0833\n",
      "Epoch 12 | Loss: 0.0789\n",
      "Epoch 13 | Loss: 0.0716\n",
      "Epoch 14 | Loss: 0.0708\n",
      "Epoch 15 | Loss: 0.0661\n",
      "Epoch 16 | Loss: 0.0653\n",
      "Epoch 17 | Loss: 0.0632\n",
      "Epoch 18 | Loss: 0.0586\n",
      "Epoch 19 | Loss: 0.0622\n",
      "Epoch 20 | Loss: 0.0597\n",
      "Epoch 21 | Loss: 0.0560\n",
      "Epoch 22 | Loss: 0.0528\n",
      "Epoch 23 | Loss: 0.0510\n",
      "Epoch 24 | Loss: 0.0540\n",
      "Epoch 25 | Loss: 0.0499\n",
      "Epoch 26 | Loss: 0.0496\n",
      "Epoch 27 | Loss: 0.0487\n",
      "Epoch 28 | Loss: 0.0485\n",
      "Epoch 29 | Loss: 0.0481\n",
      "Epoch 30 | Loss: 0.0456\n",
      "Epoch 31 | Loss: 0.0462\n"
     ]
    }
   ],
   "source": [
    "# 2. Load MNIST\n",
    "# -------------------------\n",
    "transform = transforms.ToTensor()\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Define Model (SELU + AlphaDropout)\n",
    "# -------------------------\n",
    "class MNISTAlphaDropoutNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 64),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(p=0.1),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(p=0.1),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(p=0.1),\n",
    "            nn.Linear(64, 10)  # logits\n",
    "        )\n",
    "        self.init_lecun_normal()\n",
    "\n",
    "    def init_lecun_normal(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                fan_in = layer.weight.shape[1]\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=np.sqrt(1.0 / fan_in))\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = MNISTAlphaDropoutNet()\n",
    "\n",
    "# -------------------------\n",
    "# 4. Optimizer & loss\n",
    "# -------------------------\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train normally\n",
    "# -------------------------\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Standard accuracy (dropout OFF)\n",
    "# -------------------------\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f013b",
   "metadata": {},
   "source": [
    "## Q3.1 Report the test accuracy of the network with Alpha Dropout applied during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_acc = evaluate(model)\n",
    "print(f\"Standard Test Accuracy (Dropout OFF): {base_acc:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126866b",
   "metadata": {},
   "source": [
    "## Q3.2 Report the MC Dropout-enhanced accuracy (averaging 20 stochastic predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ab5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict(model, x, mc_runs=20):\n",
    "    model.train()  # <— enable dropout at inference\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(mc_runs):\n",
    "            logits = model(x)\n",
    "            preds.append(torch.softmax(logits, dim=1))\n",
    "    return torch.stack(preds).mean(0)  # average prediction\n",
    "\n",
    "\n",
    "def evaluate_mc_dropout(model, mc_runs=20):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # we handle dropout manually inside function\n",
    "    for x, y in test_loader:\n",
    "        probs = mc_dropout_predict(model, x, mc_runs)\n",
    "        pred = probs.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "mc_acc = evaluate_mc_dropout(model)\n",
    "print(f\"MC Dropout Test Accuracy (Dropout ON, 20 passes): {mc_acc:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cac7b2",
   "metadata": {},
   "source": [
    "# Exercise-4: Transfer Learning with Pre-trained CNN \n",
    "Use a pre-trained convolutional neural network (CNN) as a feature extractor and fine-\n",
    "tune a classifier on a subset of the CIFAR-10 dataset. Set random seeds to 42. Follow\n",
    "the configuration below:\n",
    "* Load CIFAR-10 and normalize pixel values to [0,1]\n",
    "* Use only the first 2000 training samples and first 500 test samples\n",
    "* Load MobileNetV2 from tensorflow.keras.applications, with include top=False and weights=’imagenet’\n",
    "* Freeze all layers of the pre-trained base\n",
    "* Add a classifier on top:\n",
    "    * GlobalAveragePooling2D\n",
    "    * Dense layer with 128 neurons, ReLU activation\n",
    "    * Dropout: 0.2\n",
    "    * Output layer: 10 neurons with softmax\n",
    "* Optimizer: Adam, learning rate = 0.001\n",
    "* Loss: sparse categorical crossentropy\n",
    "* epochs = 5, batch size = 32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
